{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839bb893-da21-43d1-b89a-5b727998a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from QualtricsAPI.Setup import Credentials\n",
    "from QualtricsAPI.Survey import Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e482c5-5236-4d56-9b95-103b6b2a1447",
   "metadata": {},
   "source": [
    "# Data for studies 1 and 2\n",
    "\n",
    "These data have been processed in a notebook similar to this, and will now be combined with the data from study 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f129ec-7d7e-434d-86e8-9171e9f0a495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block</th>\n",
       "      <th>field</th>\n",
       "      <th>rater</th>\n",
       "      <th>binary</th>\n",
       "      <th>abstract</th>\n",
       "      <th>ordinal</th>\n",
       "      <th>rater_disc</th>\n",
       "      <th>pubyear</th>\n",
       "      <th>doi</th>\n",
       "      <th>doctype</th>\n",
       "      <th>group</th>\n",
       "      <th>chauvinism</th>\n",
       "      <th>capture</th>\n",
       "      <th>citation</th>\n",
       "      <th>socialmedia</th>\n",
       "      <th>mention</th>\n",
       "      <th>outlier</th>\n",
       "      <th>usage</th>\n",
       "      <th>present</th>\n",
       "      <th>intolerance</th>\n",
       "      <th>ethics</th>\n",
       "      <th>empirical</th>\n",
       "      <th>environment</th>\n",
       "      <th>education</th>\n",
       "      <th>wellbeing</th>\n",
       "      <th>deliverable</th>\n",
       "      <th>abstract_text</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_length</th>\n",
       "      <th>abstract_wordcount</th>\n",
       "      <th>field_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>HIST</td>\n",
       "      <td>521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>QID2_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LING</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>10.1080/09612025.2015.1028209</td>\n",
       "      <td>Article</td>\n",
       "      <td>group1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This article examines the content of Women's C...</td>\n",
       "      <td>'Our own paper': evaluating the impact of Wome...</td>\n",
       "      <td>757</td>\n",
       "      <td>115</td>\n",
       "      <td>Humanities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>HIST</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>QID2_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>PHIL</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>10.1080/09612025.2015.1028209</td>\n",
       "      <td>Article</td>\n",
       "      <td>group1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This article examines the content of Women's C...</td>\n",
       "      <td>'Our own paper': evaluating the impact of Wome...</td>\n",
       "      <td>757</td>\n",
       "      <td>115</td>\n",
       "      <td>Humanities</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  block field  rater  binary abstract  ordinal rater_disc  pubyear  \\\n",
       "0     2  HIST  521.0     0.0   QID2_1      0.0       LING   2015.0   \n",
       "1     2  HIST  142.0     0.0   QID2_1      0.0       PHIL   2015.0   \n",
       "\n",
       "                             doi  doctype   group  chauvinism  capture  \\\n",
       "0  10.1080/09612025.2015.1028209  Article  group1         0.0     30.0   \n",
       "1  10.1080/09612025.2015.1028209  Article  group1         0.0     30.0   \n",
       "\n",
       "   citation  socialmedia  mention  outlier   usage  present  intolerance  \\\n",
       "0       3.0          7.0      0.0      0.0  1021.0      0.0          1.0   \n",
       "1       3.0          7.0      0.0      0.0  1021.0      0.0          1.0   \n",
       "\n",
       "   ethics  empirical  environment  education  wellbeing  deliverable  \\\n",
       "0     0.0        0.0          0.0        0.0        0.0          0.0   \n",
       "1     0.0        0.0          0.0        0.0        0.0          0.0   \n",
       "\n",
       "                                       abstract_text  \\\n",
       "0  This article examines the content of Women's C...   \n",
       "1  This article examines the content of Women's C...   \n",
       "\n",
       "                                               title  abstract_length  \\\n",
       "0  'Our own paper': evaluating the impact of Wome...              757   \n",
       "1  'Our own paper': evaluating the impact of Wome...              757   \n",
       "\n",
       "   abstract_wordcount field_group  \n",
       "0                 115  Humanities  \n",
       "1                 115  Humanities  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    r\"C:\\Users\\conix\\Dropbox\\aWriting\\humanities impact\\data\\IIH_all_data.csv\",\n",
    "    index_col=\"Unnamed: 0\",\n",
    ")\n",
    "\n",
    "# all these raters were humanities\n",
    "df[\"Field_group\"] = \"Humanities\"\n",
    "\n",
    "# remove the test sets from the df\n",
    "# we did not record the data, so they are just NaN\n",
    "# there are 30, i.e. 2*3 sets (one for each group)\n",
    "df = df.loc[~df.binary.isna()]\n",
    "\n",
    "# make column names lower case\n",
    "df.columns = [i.lower() for i in df.columns]\n",
    "df = df.rename(columns={\"b3_l\": \"deliverable\"})\n",
    "\n",
    "# add field group columns\n",
    "df[\"field_group\"] = \"Humanities\"\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08bc4f3-f2fa-4a35-803e-785bdac32f5d",
   "metadata": {},
   "source": [
    "# Document data\n",
    "\n",
    "The abstracts, their doi, title, document type and publication year. These were downloaded from WoS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9489b7a-a6bf-444f-b188-7812dfec31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the abstracts for group3\n",
    "documents = (\n",
    "    pd.read_csv(\n",
    "        r\"C:\\Users\\conix\\Dropbox\\aWriting\\humanities impact\\follow up study\\IIH_followupdata_version3.csv\",\n",
    "        sep=\";\",\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"ORIGINAL_INDEX\"})\n",
    ")\n",
    "# keep relevant columns\n",
    "documents = documents[\n",
    "    [\"ORIGINAL_INDEX\", \"DOI\", \"TITLE\", \"ABSTRACT\", \"PUBYEAR\", \"DOCTYPE\"]\n",
    "]\n",
    "documents.columns = [i.lower() for i in documents.columns]\n",
    "documents = documents.rename(columns={\"abstract\": \"abstract_text\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c456ff-1416-4ec0-bae7-a4013e8c5585",
   "metadata": {},
   "source": [
    "# Rater data\n",
    "\n",
    "Rater codes as well as demographic information and their fields. We used codes rather than names to protect the identity of these raters. Only the member of the research team who managed the hiring (LL) knew the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8f41d50-3c54-43f2-8edc-64d6cefaba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rater data for study 3\n",
    "file_path = r\"C:\\Users\\conix\\Dropbox\\aWriting\\humanities impact\\data\"\n",
    "\n",
    "with open(file_path + \"\\\\rater_data.txt\", \"r\") as file:\n",
    "    rater_data = json.load(file)\n",
    "\n",
    "# gender and weird for studies 1 and 2 (collected later)\n",
    "# some uncertainty about 321, 342, 411, 421, 511, 521\n",
    "\n",
    "with open(file_path + \"\\\\pilot_gender.txt\", \"r\") as file:\n",
    "    pilot_gender = file.read()\n",
    "pilot_gender = ast.literal_eval(pilot_gender)\n",
    "\n",
    "with open(file_path + \"\\\\pilot_weird.txt\", \"r\") as file:\n",
    "    pilot_weird = file.read()\n",
    "pilot_weird = ast.literal_eval(pilot_weird)\n",
    "\n",
    "df[\"sex\"] = df[\"rater\"].replace(pilot_gender)\n",
    "df[\"weird\"] = df[\"rater\"].replace(pilot_weird)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf4cde6-049e-40d9-9d97-fad4cb74217d",
   "metadata": {},
   "source": [
    "# Coding data\n",
    "\n",
    "These are the topic codes for all the abstracts. These were coded by two members of the research team, then compared and discussed until consensus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c90da02a-407a-414e-9c42-66c0a3ff1bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_study3 = pd.read_csv(\n",
    "    r\"C:\\Users\\conix\\Dropbox\\aWriting\\humanities impact\\follow up study\\abstract_coding\\IIH_codingStudy2_DisagreementsExcel_corrected.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "topics_studies12 = pd.read_csv(\n",
    "    r\"C:\\Users\\conix\\Dropbox\\aWriting\\humanities impact\\follow up study\\abstract_coding\\final_codes.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "\n",
    "# check if there is no disagreement left between the two coders for study 3\n",
    "code_cols = [\n",
    "    \"A1 time\",\n",
    "    \"A2 -ism\",\n",
    "    \"A3 ethics\",\n",
    "    \"A4 emp\",\n",
    "    \"A5 world\",\n",
    "    \"A6 non-lit\",\n",
    "    \"B1 edu\",\n",
    "    \"B2 human\",\n",
    "    \"B3 deliv\",\n",
    "]\n",
    "\n",
    "for i in code_cols:\n",
    "    assert (\n",
    "        topics_study3[i + \"_LIN\"] == topics_study3[i + \"_OLI\"]\n",
    "    ).sum() == len(topics_study3), (\n",
    "        f\"col {i} has disagreements still\"\n",
    "    )\n",
    "\n",
    "# if not, then retain the codes of one rater (LIN)\n",
    "topics_study3 = topics_study3[\n",
    "    [i for i in topics_study3.columns if 'LIN' in i or 'DOI' in i]\n",
    "]\n",
    "\n",
    "# rename the columns\n",
    "column_names = [\n",
    "    \"doi\",\n",
    "    \"present\",\n",
    "    \"intolerance\",\n",
    "    \"ethics\",\n",
    "    \"empirical\",\n",
    "    \"environment\",\n",
    "    \"fiction\",\n",
    "    \"education\",\n",
    "    \"wellbeing\",\n",
    "    \"deliverable\",\n",
    "]\n",
    "\n",
    "topics_study3.columns = column_names\n",
    "\n",
    "# select the correct columns for studies 1 and 2\n",
    "topics_studies12 = topics_studies12[\n",
    "    [\n",
    "        \"DOI\",\n",
    "        \"Present\",\n",
    "        \"Intolerance\",\n",
    "        \"Ethics\",\n",
    "        \"Empirical\",\n",
    "        \"Environment\",\n",
    "        \"Fiction\",\n",
    "        \"Education\",\n",
    "        \"Wellbeing\",\n",
    "        \"Deliverable\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "#rename them too\n",
    "topics_studies12.columns = column_names\n",
    "\n",
    "# there were some duplicates in the sample, drop those, as they have the same codes\n",
    "topics_studies12 = topics_studies12.drop_duplicates(subset=\"doi\")\n",
    "\n",
    "# adapt present code for the old data (because recoded after redefinition)\n",
    "merged = df.merge(\n",
    "    topics_studies12[[\"doi\", \"present\"]], on=\"doi\", how=\"left\", suffixes=(\"\", \"_new\")\n",
    ")\n",
    "df[\"present\"] = merged[\"present_new\"].combine_first(df[\"present\"])\n",
    "\n",
    "# add fiction to the old data (because fiction introduced at study3)\n",
    "merged = df.merge(topics_studies12[[\"doi\", \"fiction\"]], on=\"doi\", how=\"left\")\n",
    "df[\"fiction\"] = merged[\"fiction\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61bcb71-ad2a-4513-be7f-414406604900",
   "metadata": {},
   "source": [
    "# Survey data\n",
    "\n",
    "## Load the data and remove what we don\"t want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdb90f4a-1576-4ea4-80e6-469c5a812173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main study data from qualtrics\n",
    "# for study 3, raters did the work in three chunks and got paid separately for each chunk.\n",
    "# Because of that, there are three separate surveys for study 3. We combine the data for these in one df.\n",
    "\n",
    "# credentials to get data via the qualtrics API\n",
    "id_s1 = \"SV_6g14RQpKZGSbx3g\"\n",
    "id_s2 = \"SV_8pGlqv9GqN2OrVI\"\n",
    "id_s3 = \"SV_bQ75Bb7jwCEnHBI\"\n",
    "\n",
    "qtoken = \"4TJ1WJofe3yHbR8duXWSNCxitHQ6d2QjyKwoZ4oz\"\n",
    "\n",
    "qdc = \"fra1\"\n",
    "\n",
    "# import data through API and store in dfs\n",
    "Credentials().qualtrics_api_credentials(token=qtoken, data_center=qdc)\n",
    "\n",
    "df1 = Responses().get_survey_responses(survey=id_s1)\n",
    "df2 = Responses().get_survey_responses(survey=id_s2)\n",
    "df3 = Responses().get_survey_responses(survey=id_s3)\n",
    "\n",
    "surveys = [df1, df2, df3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a38fb47-41a0-4b0d-b32b-3fc9f3f19277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for uploading on zenodo\n",
    "\n",
    "df1.to_csv(\n",
    "    r\"C:\\Users\\conix\\Dropbox\\aWriting\\humanities impact\\follow up study\\data_upload\\raw_group3_1_publish.csv\"\n",
    ")\n",
    "df2.to_csv(\n",
    "    r\"C:\\Users\\conix\\Dropbox\\aWriting\\humanities impact\\follow up study\\data_upload\\raw_group3_2_publish.csv\"\n",
    ")\n",
    "df3.to_csv(\n",
    "    r\"C:\\Users\\conix\\Dropbox\\aWriting\\humanities impact\\follow up study\\data_upload\\raw_group3_3_publish.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80d02a17-7513-491d-aab7-450106a6e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns we don't need to keep\n",
    "dropcols = [\n",
    "    \"RecordedDate\",\n",
    "    \"ResponseId\",\n",
    "    \"RecipientLastName\",\n",
    "    \"RecipientFirstName\",\n",
    "    \"RecipientEmail\",\n",
    "    \"ExternalReference\",\n",
    "    \"LocationLatitude\",\n",
    "    \"LocationLongitude\",\n",
    "    \"DistributionChannel\",\n",
    "    \"UserLanguage\",\n",
    "    \"Status\",\n",
    "    \"IPAddress\",\n",
    "    \"Finished\",\n",
    "    \"Progress\",\n",
    "]\n",
    "\n",
    "# strore information like when and how long raters worked\n",
    "survey_info = {\"durations\": [], \"dates\": [], \"block_times\": [], \"titles\": []}\n",
    "\n",
    "\n",
    "def clean_surveys(survey):\n",
    "\n",
    "    # store titles for the abstracts\n",
    "    pattern = r\"as well. - (.*?)\\n\\n\\n\\n\"\n",
    "    survey_info[\"titles\"].append(\n",
    "        [\n",
    "            survey[col].str.extract(pattern).iloc[0, 0]\n",
    "            for col in survey.columns\n",
    "            if ((\"TEXT\" in col) and (\"QID\" in col))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # only finished surveys\n",
    "    survey = survey.loc[survey.Progress == \"100\"]\n",
    "\n",
    "    # drop unneeded columns across surveys\n",
    "    survey = survey.drop(columns=dropcols)\n",
    "\n",
    "    # remove the timer columns except for total block time\n",
    "    timer_cols = [\n",
    "        i for i in survey.columns if ((\"First\" in i) or (\"Last\" in i) or (\"Count\" in i))\n",
    "    ]\n",
    "    survey = survey.drop(columns=timer_cols)\n",
    "\n",
    "    # remove test surveys from research team\n",
    "    researchers = [\"stijn\", \"lin\", \"leander\", \"olivier\", \"pei-shan\"]\n",
    "    pattern = \"|\".join(researchers)\n",
    "    survey = survey[\n",
    "        ~survey[\"QID0\"]\n",
    "        .str.replace(\"[^a-zA-Z]\", \"\", regex=True)\n",
    "        .str.contains(pattern, case=False, na=False)\n",
    "    ]\n",
    "\n",
    "    # store and remove duration\n",
    "    survey_info[\"durations\"].append(survey[[\"QID0\", \"Duration (in seconds)\"]])\n",
    "    survey = survey.drop(columns=[\"Duration (in seconds)\"])\n",
    "\n",
    "    # store and remove dates\n",
    "    survey_info[\"dates\"].append(survey[[\"QID0\", \"StartDate\", \"EndDate\"]])\n",
    "    survey = survey.drop(columns=[\"StartDate\", \"EndDate\"])\n",
    "\n",
    "    # store and remove block times\n",
    "    block_times = [i for i in survey.columns if \"QT\" in i] + [\"QID0\"]\n",
    "    survey_info[\"block_times\"].append(survey[block_times])\n",
    "    survey = survey[[i for i in survey.columns if \"QT\" not in i]]\n",
    "\n",
    "    # rename QID0 column to ID\n",
    "    survey = survey.rename(columns={\"QID0\": \"rater\"})\n",
    "\n",
    "    return survey\n",
    "\n",
    "\n",
    "df1 = clean_surveys(df1)\n",
    "df2 = clean_surveys(df2)\n",
    "df3 = clean_surveys(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f42280d4-8f45-43c5-9ef0-394a2df8f646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid value found in set 0 at Row: 17, Column: 'QID5_1_TEXT' -> Value: 2\n"
     ]
    }
   ],
   "source": [
    "# There was one rater thrown out because of inconsistencies\n",
    "# we remove them\n",
    "\n",
    "df1 = df1.loc[df1.rater != \"211\"]\n",
    "\n",
    "# check if all surveys have the same number of raters\n",
    "assert (\n",
    "    len(df1) == len(df2) == len(df3)\n",
    "), \"not all surveys have the same number of raters\"\n",
    "\n",
    "# check if all binary data is indeed binary\n",
    "for set, i in enumerate([df1, df2, df3]):\n",
    "    binary_cols = [col for col in i.columns if \"TEXT\" in col]\n",
    "    binary_df = i[binary_cols].copy()\n",
    "    invalid_values_mask = binary_df.map(lambda x: x not in [\"0\", \"1\"])\n",
    "    invalid_locations = np.where(invalid_values_mask)\n",
    "    positions = list(zip(invalid_locations[0], invalid_locations[1]))\n",
    "    for pos in positions:\n",
    "        row, col = pos\n",
    "        print(\n",
    "            f\"Invalid value found in set {set} at Row: {row}, Column: '{binary_df.columns[col]}' -> Value: {binary_df.iloc[row, col]}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# fix the value that is problematic\n",
    "# we assume that the 2 is supposed to be a 1\n",
    "\n",
    "df1.loc[df1.index[17], \"QID5_1_TEXT\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee63c550-8d9c-4cac-9b8a-75ff94add7cd",
   "metadata": {},
   "source": [
    "## Turn into DF with ratings as rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bd7e630-bf16-40c4-888b-487742faa2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df with as the rows ratings, an as the columns the field, rater, block, rank score and binary score\n",
    "def transform_survey_data(df, set):\n",
    "    df_final = pd.DataFrame(columns=[\"rater\", \"block\", \"field\", \"ordinal\", \"binary\"])\n",
    "\n",
    "    # Iterate through each QID set\n",
    "    for qid in range(1, 11):  # each set has 10 blocks, we remove the burnin\n",
    "        for position in range(1, 6):  # five papers per block\n",
    "            # Extract relevant columns for the current position and task\n",
    "            rank_col = f\"QID{qid}_{position}\"\n",
    "            binary_col = f\"{rank_col}_TEXT\"\n",
    "            temp_df = df[[\"rater\", rank_col, binary_col]].copy()\n",
    "            temp_df.rename(\n",
    "                columns={rank_col: \"ordinal\", binary_col: \"binary\"}, inplace=True\n",
    "            )\n",
    "            temp_df[\"block\"] = f\"QID{qid}_{set}\"\n",
    "            temp_df[\"field\"] = position\n",
    "            # Append to the final DataFrame\n",
    "            df_final = pd.concat([df_final, temp_df], ignore_index=True)\n",
    "\n",
    "            # add a column to indicate that these data are not pilot data\n",
    "            df_final[\"group\"] = \"main\"\n",
    "\n",
    "    return df_final\n",
    "\n",
    "\n",
    "df1 = transform_survey_data(df1, 1)\n",
    "df2 = transform_survey_data(df2, 2)\n",
    "df3 = transform_survey_data(df3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "125de329-4864-43fc-84e4-e1e459027407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the titles of the abstracts to each row\n",
    "\n",
    "n_raters = len(df1.rater.unique())\n",
    "\n",
    "df1[\"title\"] = np.repeat(\n",
    "    np.array(survey_info[\"titles\"])[0], n_raters  # get the stored titles\n",
    ")  # tile them n_raters times\n",
    "df2[\"title\"] = np.repeat(\n",
    "    np.array(survey_info[\"titles\"])[1], n_raters  # get the stored titles\n",
    ")  # tile them n_raters times\n",
    "df3[\"title\"] = np.repeat(\n",
    "    np.array(survey_info[\"titles\"])[2], n_raters  # get the stored titles\n",
    ")  # tile them n_raters times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518aa17c-1bbd-4a6f-8e30-07d721967c20",
   "metadata": {},
   "source": [
    "# Combine with other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43ff15ff-150e-47a9-b662-b7b612a14e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rater</th>\n",
       "      <th>block</th>\n",
       "      <th>field</th>\n",
       "      <th>ordinal</th>\n",
       "      <th>binary</th>\n",
       "      <th>group</th>\n",
       "      <th>title</th>\n",
       "      <th>rater_disc</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Sex</th>\n",
       "      <th>WEIRD</th>\n",
       "      <th>Field_group</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>151_main</td>\n",
       "      <td>QID1_1_main</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>main</td>\n",
       "      <td>Does China Matter? Taiwan's Successful Bid to ...</td>\n",
       "      <td>History</td>\n",
       "      <td>Argentinian</td>\n",
       "      <td>M</td>\n",
       "      <td>not WEIRD</td>\n",
       "      <td>Humanities</td>\n",
       "      <td>QID1_1_main_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111_main</td>\n",
       "      <td>QID1_1_main</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>main</td>\n",
       "      <td>Does China Matter? Taiwan's Successful Bid to ...</td>\n",
       "      <td>Philosophy</td>\n",
       "      <td>Serbian</td>\n",
       "      <td>M</td>\n",
       "      <td>not WEIRD</td>\n",
       "      <td>Humanities</td>\n",
       "      <td>QID1_1_main_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>121_main</td>\n",
       "      <td>QID1_1_main</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>main</td>\n",
       "      <td>Does China Matter? Taiwan's Successful Bid to ...</td>\n",
       "      <td>Linguistics</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>M</td>\n",
       "      <td>not WEIRD</td>\n",
       "      <td>Humanities</td>\n",
       "      <td>QID1_1_main_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rater        block  field  ordinal  binary group  \\\n",
       "0  151_main  QID1_1_main      1        3       0  main   \n",
       "1  111_main  QID1_1_main      1        4       1  main   \n",
       "2  121_main  QID1_1_main      1        1       1  main   \n",
       "\n",
       "                                               title   rater_disc  \\\n",
       "0  Does China Matter? Taiwan's Successful Bid to ...      History   \n",
       "1  Does China Matter? Taiwan's Successful Bid to ...   Philosophy   \n",
       "2  Does China Matter? Taiwan's Successful Bid to ...  Linguistics   \n",
       "\n",
       "   Nationality Sex      WEIRD Field_group       abstract  \n",
       "0  Argentinian   M  not WEIRD  Humanities  QID1_1_main_1  \n",
       "1      Serbian   M  not WEIRD  Humanities  QID1_1_main_1  \n",
       "2      Turkish   M  not WEIRD  Humanities  QID1_1_main_1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, combine all three sets, such that we have all data for study 3 combined in one df\n",
    "main_study = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# add rater data\n",
    "def map_values(row):\n",
    "    # Extract ID from the row\n",
    "    id_value = row[\"rater\"]\n",
    "    # Look up the inner dictionary using this ID\n",
    "    inner_dict = rater_data.get(id_value, {})\n",
    "    # For each key in the inner dictionary, set the row\"s corresponding column\n",
    "    for key, value in inner_dict.items():\n",
    "        row[key] = value\n",
    "    return row\n",
    "\n",
    "main_study = main_study.apply(map_values, axis=1)\n",
    "\n",
    "# add main to rater column and block column\n",
    "# this way, we avoid confusion with data from studies 1 & 2\n",
    "main_study[\"block\"] = [f\"{i}_main\" for i in main_study.block.values]\n",
    "main_study[\"rater\"] = [f\"{i}_main\" for i in main_study.rater.values]\n",
    "\n",
    "# give the abstracts unique names\n",
    "# use the field position and block\n",
    "main_study[\"abstract\"] = main_study[\"field\"].replace(\n",
    "    {\n",
    "        \"History\": \"1\",\n",
    "        \"Philosophy\": \"2\",\n",
    "        \"Religion\": \"3\",\n",
    "        \"Linguistics\": \"4\",\n",
    "        \"Literature\": \"5\",\n",
    "    }\n",
    ")\n",
    "main_study[\"abstract\"] = (\n",
    "    main_study[\"block\"] + \"_\" + main_study[\"abstract\"].astype(\"string\")\n",
    ")\n",
    "\n",
    "# make ordinal and binary integer\n",
    "for i in [\"binary\", \"ordinal\"]:\n",
    "    main_study[i] = main_study[i].astype(\"int32\")\n",
    "\n",
    "# replace 'Theology' by 'Religion' for consistency\n",
    "main_study[\"rater_disc\"] = main_study[\"rater_disc\"].replace({\"Theology\": \"Religion\"})\n",
    "\n",
    "main_study.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d48153ec-4049-4f28-9337-fa2c88a90835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add documents\n",
    "# one problem in the merging: one of the titles had a character that had been changed\n",
    "# was in the title: \"God as a true Elohim and Savior of the Poor - Psalm 82 in the Corpus of the Psalms of Asaph\"\n",
    "# replace with the string from the survey\n",
    "\n",
    "documents.loc[documents.title.str.contains(\"Psalm 82\"), \"title\"] = (\n",
    "    \"God as a true Elohim and Savior of the Poor - Psalm 82 in the Corpus of the Psalms of Asaph\"\n",
    ")\n",
    "\n",
    "main_study = documents.merge(main_study, on=\"title\", how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d67cdb6-85ab-4933-a413-d8382576c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raters were kicked if they had too many inconsistencies. If they had 1, we allowed them to fix it.\n",
    "# here we implement the fixes\n",
    "\n",
    "data = {}\n",
    "\n",
    "# 7 mistakes in total, 1 in each of the following sets\n",
    "fixes = [\"111\", \"111_set3\", \"121\", \"131\", \"151\", \"212\", \"226_set3\"]\n",
    "\n",
    "for i in fixes:\n",
    "    # read the fixed data\n",
    "    fixed_data = pd.read_csv(\n",
    "        r\"C:\\Users\\conix\\Dropbox\\aWriting\\humanities impact\\follow up study\\raters_fix\\\\\"\n",
    "        + i\n",
    "        + \"_corrected.csv\",\n",
    "        dtype={\"ranks\": \"int32\", \"binary\": \"int32\"},\n",
    "        sep=\";\",\n",
    "    )\n",
    "\n",
    "    # name the columns of the fixed data\n",
    "    fixed_data.columns = [\n",
    "        \"index_col\",\n",
    "        \"title\",\n",
    "        \"abstract_text\",\n",
    "        \"set\",\n",
    "        \"ordinal\",\n",
    "        \"binary\",\n",
    "    ]\n",
    "\n",
    "    # check whether it is set 1 or set 3\n",
    "    if \"_\" in i:\n",
    "        rater = i.split(\"_\")[0]\n",
    "        block = i.split(\"set\")[1]\n",
    "    else:\n",
    "        rater = i\n",
    "        block = \"1\"\n",
    "        fixed_data = fixed_data.loc[~fixed_data.set.isin([\"set_a\", \"set_b\", \"set_c\"])]\n",
    "\n",
    "    # get the relevant columns\n",
    "    fixed_data = fixed_data[[\"title\", \"ordinal\", \"binary\"]]\n",
    "\n",
    "    condition = (main_study.rater == rater + \"_main\") & (\n",
    "        main_study.block.str.contains(\"_\" + block + \"_main\")\n",
    "    )\n",
    "\n",
    "    # get the relevant data\n",
    "    main_study_filtered = main_study.copy().loc[condition]\n",
    "\n",
    "    # combine the fixed and original values in one df for comparison\n",
    "    merged_df = pd.merge(\n",
    "        main_study_filtered, fixed_data, on=\"title\", how=\"left\", suffixes=(\"_old\", \"\")\n",
    "    )\n",
    "\n",
    "    # replace the originally recorded value by the fixed value\n",
    "    for col in [\"binary\", \"ordinal\"]:\n",
    "        main_study.loc[condition, col] = list(merged_df[col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9c90803-7f30-4113-8882-4cbb29ac26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse ordinal data (to make 'more valuable' a higher score, and 'less valuable' a lower score)\n",
    "# We have to do this because in the survey the most valuable was the document ranked 1, and the least was ranked 5.\n",
    "main_study[\"ordinal\"] = main_study[\"ordinal\"].astype(\"int32\")\n",
    "main_study[\"ordinal\"] = main_study[\"ordinal\"].replace({1: 5, 2: 4, 3: 3, 4: 2, 5: 1})\n",
    "\n",
    "# make it start at 0 rather than 1\n",
    "main_study[\"ordinal\"] = main_study[\"ordinal\"] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c9a41f5-d7db-4087-aef2-40c6a625f311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_index</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_text</th>\n",
       "      <th>pubyear</th>\n",
       "      <th>doctype</th>\n",
       "      <th>rater</th>\n",
       "      <th>block</th>\n",
       "      <th>field</th>\n",
       "      <th>ordinal</th>\n",
       "      <th>binary</th>\n",
       "      <th>group</th>\n",
       "      <th>rater_disc</th>\n",
       "      <th>nationality</th>\n",
       "      <th>sex</th>\n",
       "      <th>weird</th>\n",
       "      <th>field_group</th>\n",
       "      <th>abstract</th>\n",
       "      <th>chauvinism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.1080/09523367.2015.1022721</td>\n",
       "      <td>Does China Matter? Taiwan's Successful Bid to ...</td>\n",
       "      <td>This study seeks to identify and explain the k...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Article</td>\n",
       "      <td>151_main</td>\n",
       "      <td>QID1_1_main</td>\n",
       "      <td>History</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>main</td>\n",
       "      <td>History</td>\n",
       "      <td>Argentinian</td>\n",
       "      <td>M</td>\n",
       "      <td>not WEIRD</td>\n",
       "      <td>Humanities</td>\n",
       "      <td>QID1_1_main_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10.1080/09523367.2015.1022721</td>\n",
       "      <td>Does China Matter? Taiwan's Successful Bid to ...</td>\n",
       "      <td>This study seeks to identify and explain the k...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Article</td>\n",
       "      <td>111_main</td>\n",
       "      <td>QID1_1_main</td>\n",
       "      <td>History</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>main</td>\n",
       "      <td>Philosophy</td>\n",
       "      <td>Serbian</td>\n",
       "      <td>M</td>\n",
       "      <td>not WEIRD</td>\n",
       "      <td>Humanities</td>\n",
       "      <td>QID1_1_main_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>10.1080/09523367.2015.1022721</td>\n",
       "      <td>Does China Matter? Taiwan's Successful Bid to ...</td>\n",
       "      <td>This study seeks to identify and explain the k...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Article</td>\n",
       "      <td>121_main</td>\n",
       "      <td>QID1_1_main</td>\n",
       "      <td>History</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>main</td>\n",
       "      <td>Linguistics</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>M</td>\n",
       "      <td>not WEIRD</td>\n",
       "      <td>Humanities</td>\n",
       "      <td>QID1_1_main_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_index                            doi  \\\n",
       "0               0  10.1080/09523367.2015.1022721   \n",
       "1               0  10.1080/09523367.2015.1022721   \n",
       "2               0  10.1080/09523367.2015.1022721   \n",
       "\n",
       "                                               title  \\\n",
       "0  Does China Matter? Taiwan's Successful Bid to ...   \n",
       "1  Does China Matter? Taiwan's Successful Bid to ...   \n",
       "2  Does China Matter? Taiwan's Successful Bid to ...   \n",
       "\n",
       "                                       abstract_text  pubyear  doctype  \\\n",
       "0  This study seeks to identify and explain the k...     2015  Article   \n",
       "1  This study seeks to identify and explain the k...     2015  Article   \n",
       "2  This study seeks to identify and explain the k...     2015  Article   \n",
       "\n",
       "      rater        block    field  ordinal  binary group   rater_disc  \\\n",
       "0  151_main  QID1_1_main  History        2       0  main      History   \n",
       "1  111_main  QID1_1_main  History        1       1  main   Philosophy   \n",
       "2  121_main  QID1_1_main  History        4       1  main  Linguistics   \n",
       "\n",
       "   nationality sex      weird field_group       abstract  chauvinism  \n",
       "0  Argentinian   M  not WEIRD  Humanities  QID1_1_main_1           1  \n",
       "1      Serbian   M  not WEIRD  Humanities  QID1_1_main_1           0  \n",
       "2      Turkish   M  not WEIRD  Humanities  QID1_1_main_1           0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add field strings\n",
    "field_codes = {\n",
    "    1: \"History\",\n",
    "    2: \"Philosophy\",\n",
    "    3: \"Religion\",\n",
    "    4: \"Linguistics\",\n",
    "    5: \"Literature\",\n",
    "}\n",
    "main_study[\"field\"] = main_study[\"field\"].replace(field_codes)\n",
    "\n",
    "# add chauvinism, i.e. rater's field is same as abstract field\n",
    "main_study[\"chauvinism\"] = main_study.apply(\n",
    "    lambda row: 1 if row[\"field\"] == row[\"rater_disc\"] else 0, axis=1\n",
    ")\n",
    "\n",
    "# make column names lower case\n",
    "main_study.columns = [i.lower() for i in main_study.columns]\n",
    "\n",
    "main_study.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c495d4b-dae5-40cc-8e86-05d72c293d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add topic codes\n",
    "main_study = pd.merge(\n",
    "    main_study, topics_study3, on=\"doi\", how=\"left\", suffixes=(\"\", \"_from_df2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2718a16-0256-4d7c-9a7d-0cd80a13beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eventually add altmetrics\n",
    "# data has been downloaded, but we wait for preregistration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77f692ac-34bc-446b-9fac-8f26b00045b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of document ratings: 8820\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_index</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_text</th>\n",
       "      <th>pubyear</th>\n",
       "      <th>doctype</th>\n",
       "      <th>rater</th>\n",
       "      <th>block</th>\n",
       "      <th>field</th>\n",
       "      <th>ordinal</th>\n",
       "      <th>binary</th>\n",
       "      <th>group</th>\n",
       "      <th>rater_disc</th>\n",
       "      <th>nationality</th>\n",
       "      <th>sex</th>\n",
       "      <th>weird</th>\n",
       "      <th>field_group</th>\n",
       "      <th>abstract</th>\n",
       "      <th>chauvinism</th>\n",
       "      <th>present</th>\n",
       "      <th>intolerance</th>\n",
       "      <th>ethics</th>\n",
       "      <th>empirical</th>\n",
       "      <th>environment</th>\n",
       "      <th>fiction</th>\n",
       "      <th>education</th>\n",
       "      <th>wellbeing</th>\n",
       "      <th>deliverable</th>\n",
       "      <th>capture</th>\n",
       "      <th>citation</th>\n",
       "      <th>socialmedia</th>\n",
       "      <th>mention</th>\n",
       "      <th>outlier</th>\n",
       "      <th>usage</th>\n",
       "      <th>abstract_length</th>\n",
       "      <th>abstract_wordcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1080/09523367.2015.1022721</td>\n",
       "      <td>Does China Matter? Taiwan's Successful Bid to ...</td>\n",
       "      <td>This study seeks to identify and explain the k...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Article</td>\n",
       "      <td>151_main</td>\n",
       "      <td>QID1_1_main</td>\n",
       "      <td>History</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>main</td>\n",
       "      <td>History</td>\n",
       "      <td>Argentinian</td>\n",
       "      <td>M</td>\n",
       "      <td>not WEIRD</td>\n",
       "      <td>Humanities</td>\n",
       "      <td>QID1_1_main_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1080/09523367.2015.1022721</td>\n",
       "      <td>Does China Matter? Taiwan's Successful Bid to ...</td>\n",
       "      <td>This study seeks to identify and explain the k...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Article</td>\n",
       "      <td>111_main</td>\n",
       "      <td>QID1_1_main</td>\n",
       "      <td>History</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>main</td>\n",
       "      <td>Philosophy</td>\n",
       "      <td>Serbian</td>\n",
       "      <td>M</td>\n",
       "      <td>not WEIRD</td>\n",
       "      <td>Humanities</td>\n",
       "      <td>QID1_1_main_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1080/09523367.2015.1022721</td>\n",
       "      <td>Does China Matter? Taiwan's Successful Bid to ...</td>\n",
       "      <td>This study seeks to identify and explain the k...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Article</td>\n",
       "      <td>121_main</td>\n",
       "      <td>QID1_1_main</td>\n",
       "      <td>History</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>main</td>\n",
       "      <td>Linguistics</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>M</td>\n",
       "      <td>not WEIRD</td>\n",
       "      <td>Humanities</td>\n",
       "      <td>QID1_1_main_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_index                            doi  \\\n",
       "0             0.0  10.1080/09523367.2015.1022721   \n",
       "1             0.0  10.1080/09523367.2015.1022721   \n",
       "2             0.0  10.1080/09523367.2015.1022721   \n",
       "\n",
       "                                               title  \\\n",
       "0  Does China Matter? Taiwan's Successful Bid to ...   \n",
       "1  Does China Matter? Taiwan's Successful Bid to ...   \n",
       "2  Does China Matter? Taiwan's Successful Bid to ...   \n",
       "\n",
       "                                       abstract_text  pubyear  doctype  \\\n",
       "0  This study seeks to identify and explain the k...   2015.0  Article   \n",
       "1  This study seeks to identify and explain the k...   2015.0  Article   \n",
       "2  This study seeks to identify and explain the k...   2015.0  Article   \n",
       "\n",
       "      rater        block    field  ordinal  binary group   rater_disc  \\\n",
       "0  151_main  QID1_1_main  History      2.0     0.0  main      History   \n",
       "1  111_main  QID1_1_main  History      1.0     1.0  main   Philosophy   \n",
       "2  121_main  QID1_1_main  History      4.0     1.0  main  Linguistics   \n",
       "\n",
       "   nationality sex      weird field_group       abstract  chauvinism  present  \\\n",
       "0  Argentinian   M  not WEIRD  Humanities  QID1_1_main_1         1.0      1.0   \n",
       "1      Serbian   M  not WEIRD  Humanities  QID1_1_main_1         0.0      1.0   \n",
       "2      Turkish   M  not WEIRD  Humanities  QID1_1_main_1         0.0      1.0   \n",
       "\n",
       "   intolerance  ethics  empirical  environment  fiction  education  wellbeing  \\\n",
       "0          0.0     0.0        0.0          0.0      1.0        0.0        0.0   \n",
       "1          0.0     0.0        0.0          0.0      1.0        0.0        0.0   \n",
       "2          0.0     0.0        0.0          0.0      1.0        0.0        0.0   \n",
       "\n",
       "   deliverable  capture  citation  socialmedia  mention  outlier  usage  \\\n",
       "0          0.0      NaN       NaN          NaN      NaN      NaN    NaN   \n",
       "1          0.0      NaN       NaN          NaN      NaN      NaN    NaN   \n",
       "2          0.0      NaN       NaN          NaN      NaN      NaN    NaN   \n",
       "\n",
       "   abstract_length  abstract_wordcount  \n",
       "0              NaN                 NaN  \n",
       "1              NaN                 NaN  \n",
       "2              NaN                 NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the data for studies 1 and 2 on the one hand, and study 3 (processed up to here) on the other hand\n",
    "combined_df = pd.concat([main_study, df], ignore_index=True, sort=False)\n",
    "\n",
    "# make all columns lower case\n",
    "combined_df.columns = combined_df.columns.str.lower()\n",
    "\n",
    "print(f\"Total number of document ratings: {len(combined_df)}\")\n",
    "combined_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cc784b9-82c8-453f-9dd6-692b42f6abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# homogenize field term usage\n",
    "disc_names = {\n",
    "    \"HIST\": \"History\",\n",
    "    \"PHIL\": \"Philosophy\",\n",
    "    \"LING\": \"Linguistics\",\n",
    "    \"LIT\": \"Literature\",\n",
    "    \"REL\": \"Religion\",\n",
    "    \"Theology\": \"Religion\",\n",
    "}\n",
    "combined_df = combined_df.replace(disc_names)\n",
    "\n",
    "# homogenize rater names\n",
    "combined_df[\"rater\"] = combined_df[\"rater\"].astype(\"str\")\n",
    "combined_df[\"rater\"] = [i.split(\".\")[0] for i in combined_df[\"rater\"].values]\n",
    "\n",
    "# get abstract length (normalized)  and word count\n",
    "combined_df[\"abstract_length\"] = [\n",
    "    len(i) for i in list(combined_df[\"abstract_text\"].values)\n",
    "]\n",
    "combined_df[\"abstract_length_norm\"] = (\n",
    "    combined_df.abstract_length.values - combined_df.abstract_length.values.mean()\n",
    ") / np.std(combined_df.abstract_length.values)\n",
    "\n",
    "combined_df[\"abstract_wordcount\"] = [\n",
    "    len(i.split(\" \")) for i in combined_df[\"abstract_text\"].values\n",
    "]\n",
    "\n",
    "# add columns indicating whether the rater was from humanities\n",
    "combined_df[\"humanities\"] = np.where(combined_df.field_group == \"Humanities\", 1, 0)\n",
    "\n",
    "# add a rater disc where all non-hum raters are described as that\n",
    "combined_df[\"new_rater_disc\"] = combined_df.apply(\n",
    "    lambda row: (\n",
    "        row[\"rater_disc\"] if row[\"field_group\"] == \"Humanities\" else \"non-humanities\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n",
    "# turn columns that can into int\n",
    "int_cols = [\n",
    "    \"ordinal\",\n",
    "    \"binary\",\n",
    "    \"pubyear\",\n",
    "    \"chauvinism\",\n",
    "    \"present\",\n",
    "    \"intolerance\",\n",
    "    \"ethics\",\n",
    "    \"empirical\",\n",
    "    \"environment\",\n",
    "    \"fiction\",\n",
    "    \"education\",\n",
    "    \"wellbeing\",\n",
    "    \"deliverable\",\n",
    "]\n",
    "combined_df[int_cols] = combined_df[int_cols].astype(\"float64\")\n",
    "combined_df[int_cols] = combined_df[int_cols].astype(\"Int64\")\n",
    "\n",
    "# turn columns that can into categorical\n",
    "cat_cols = [\"sex\", \"weird\", \"rater\", \"field\", \"new_rater_disc\", \"abstract\", \"doctype\"]\n",
    "combined_df[cat_cols] = combined_df[cat_cols].astype(\"category\")\n",
    "\n",
    "# create df for the main study\n",
    "# remove unused categories in the categorical columns\n",
    "df_main = combined_df.loc[combined_df.group == \"main\"].copy()\n",
    "for col in cat_cols:\n",
    "    df_main[col] = df_main[col].cat.remove_unused_categories()\n",
    "\n",
    "# make ranks start at 1 again\n",
    "combined_df[\"ordinal\"] = combined_df[\"ordinal\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cea06693-8b63-4f6d-ad43-0d599ace2c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data for use in analysis\n",
    "\n",
    "combined_df.to_csv(\n",
    "    r\"C:\\Users\\conix\\Dropbox\\aWriting\\humanities impact\\follow up study\\combined_data.csv\"\n",
    ")\n",
    "df_main.to_csv(\n",
    "    r\"C:\\Users\\conix\\Dropbox\\aWriting\\humanities impact\\follow up study\\df_main.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ab93f49-b6c2-4b98-a04d-71f4eeeb3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove almterics data before uploading the processed data\n",
    "\n",
    "combined_df2 = combined_df[\n",
    "    [\n",
    "        i\n",
    "        for i in combined_df.columns\n",
    "        if i\n",
    "        not in [\"capture\", \"citation\", \"socialmedia\", \"usage\", \"mention\", \"outlier\"]\n",
    "    ]\n",
    "]\n",
    "combined_df2.to_csv(\n",
    "    r\"C:\\Users\\conix\\Dropbox\\aWriting\\humanities impact\\follow up study\\data_upload\\combined_data_for_upload.csv\",\n",
    "    sep=\";\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9899000-0342-41ee-94d6-b835dfb2ca4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
